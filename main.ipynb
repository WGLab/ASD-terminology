{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, string\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_empty(DIRECTORY, filename):\n",
    "    with open(os.path.join(DIRECTORY, filename)) as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    return data.isspace() or data == \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(pred_df, true_df, match_cui=False):\n",
    "    \n",
    "    # count overlap and CUI match\n",
    "    if match_cui:\n",
    "        match_grouped = pred_df.merge(true_df, on=[\"paper\", \"CUI\"], how=\"outer\")\n",
    "        # drop duplicate predictions on same entity span and CUI\n",
    "        pred_df = pred_df.drop_duplicates(subset=[\"paper\", \"CUI\", \"Start\", \"End\"])\n",
    "        true_df = true_df.drop_duplicates(subset=[\"paper\", \"CUI\", \"Start\", \"End\"])\n",
    "        \n",
    "    # count overlap only - get true positives\n",
    "    else:\n",
    "        match_grouped = pred_df.merge(true_df, on=[\"paper\"], how=\"outer\")\n",
    "        # drop duplicate predictions on same entity span\n",
    "        pred_df = pred_df.drop_duplicates(subset=[\"paper\", \"Start\", \"End\"])\n",
    "        true_df = true_df.drop_duplicates(subset=[\"paper\", \"Start\", \"End\"])\n",
    "    \n",
    "    match_grouped = match_grouped.rename(columns={\"Start_x\": \"Start_pred\", \"End_x\": \"End_pred\", \"Start_y\": \"Start_label\", \"End_y\": \"End_label\", \"Entity_x\": \"Entity_pred\", \"Entity_y\": \"Entity_label\"})\n",
    "    match_grouped = match_grouped.fillna(\"NA\")\n",
    "    # count overlaps\n",
    "    temp = match_grouped[(match_grouped[\"Start_pred\"] != \"NA\") & (match_grouped[\"Start_label\"] != \"NA\")]\n",
    "    temp = temp[((temp[\"Start_pred\"] >= temp[\"Start_label\"]) & (temp[\"Start_pred\"] <= temp[\"End_label\"])) | ((temp[\"Start_label\"] >= temp[\"Start_pred\"]) & (temp[\"Start_label\"] <= temp[\"End_pred\"]))]   \n",
    "    true_pos_df = temp\n",
    "\n",
    "    num_true_pos = len(temp.drop_duplicates([\"paper\", \"Start_label\", \"End_label\"])) # only count max one pred per label\n",
    "    num_label_pos = len(true_df)\n",
    "    num_pred_pos = len(pred_df)\n",
    "\n",
    "    print(\"Number of true positives =\", num_true_pos)\n",
    "    print(\"Number of positive labels =\", num_label_pos)\n",
    "    print(\"Number of positive predictions =\", num_pred_pos)\n",
    "    print()\n",
    "    precision = num_true_pos/num_pred_pos\n",
    "    recall = num_true_pos/num_label_pos\n",
    "    print(\"Precision =\", 100 * precision)\n",
    "    print(\"Recall =\", 100 * recall)\n",
    "    print(\"F-Measure =\", (2 * precision * recall) / (precision + recall))\n",
    "    \n",
    "    return true_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pred(pred_df_temp, remove_non_asd=False, cui=True):\n",
    "    \n",
    "    if cui:\n",
    "        # valid CUI only\n",
    "        pred_df_temp = pred_df_temp[(pred_df_temp[\"CUI\"].str.len() == 8) & (pred_df_temp[\"CUI\"].str[0] == 'C')] \n",
    "    \n",
    "    # strip entity\n",
    "    pred_df_temp[\"Entity\"] = pred_df_temp[\"Entity\"].str.strip()\n",
    "    \n",
    "    # remove non-asd specific terms\n",
    "    if remove_non_asd:\n",
    "        autism_comorbid = set(pd.read_csv(\"not-asd-specific.csv\")[\"Entity\"])\n",
    "        entities = set(pred_df_temp[\"Entity\"])\n",
    "        remove = set()\n",
    "        # remove entities that contain comorbid term\n",
    "        for e in entities:\n",
    "            for c in autism_comorbid:\n",
    "                if str(c) in str(e) or str(c) in str(e).lower():\n",
    "                    remove.add(e)\n",
    "                    \n",
    "        pred_df_temp = pred_df_temp[~pred_df_temp[\"Entity\"].isin(remove)]\n",
    "        \n",
    "        \n",
    "    pred_df = pred_df_temp\n",
    "    \n",
    "    print(\"Number of entities predicted =\", len(pred_df))\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_true(true_temp_df, gold_standard, cui_to_tui):\n",
    "    \n",
    "    # remove nan\n",
    "    true_temp_df = true_temp_df[true_temp_df[\"Entity_lower\"].str.lower() != \"nan\"]\n",
    "    \n",
    "    # double check all entities are in gold standard\n",
    "    for e in set(true_temp_df[\"Entity_lower\"]):\n",
    "        if not e in set(gold_standard[\"TEXT\"]):\n",
    "            print(e)\n",
    "\n",
    "    # merge entity with CUI\n",
    "    true_df = true_temp_df.merge(gold_standard, left_on=\"Entity_lower\", right_on=\"TEXT\", how=\"inner\")\n",
    "    #true_df = true_df[(true_df[\"CUI\"].str.len() == 8) & (true_df[\"CUI\"].str[0] == 'C')] # valid CUI ID only\n",
    "    \n",
    "    # merge cui with TUI\n",
    "    true_df = true_df.merge(cui_to_tui, on=\"CUI\", how=\"left\")\n",
    "    \n",
    "    print(\"Number of entities labelled =\", len(true_df))\n",
    "    return true_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CUI(x):\n",
    "    \n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    else:\n",
    "        return x.split()[0].strip()\n",
    "    \n",
    "def remove_stop(s):\n",
    "    s = str(s)\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "    spl = s.split(\" \", 1)\n",
    "    if spl[0] in spacy_stopwords and len(spl) > 1:\n",
    "        return spl[1]\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_grouped(df, write_to_file=False, filename=None):\n",
    "    pred_df = df\n",
    "    grouped_paper_pred = pred_df.groupby(by=[\"CUI\", \"Entity\"])[\"paper\"].nunique().reset_index().sort_values('paper', ascending=False)\n",
    "    grouped_pred = pred_df.groupby(by=[\"CUI\", \"Entity\"])[\"Start\"].count().reset_index().sort_values('Start', ascending=False)\n",
    "    grouped_pred = grouped_pred.merge(grouped_paper_pred, on=[\"CUI\", \"Entity\"])\n",
    "    grouped_pred.columns = [\"CUI\", \"Entity\", \"count\", \"num_papers\"]\n",
    "    \n",
    "    if write_to_file:\n",
    "        # write results to text file\n",
    "        with open(filename, \"w\") as f:\n",
    "            print(\"CUI\", \"Entity\", \"count\", \"num_papers\", sep=\"\\t\", file=f)\n",
    "            for index, row in grouped_pred.iterrows():\n",
    "                print(str(row[\"CUI\"]), str(row[\"Entity\"]), str(row[\"count\"]), str(row[\"num_papers\"]), sep=\"\\t\", file=f)\n",
    "    \n",
    "    return grouped_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_false_and_true_pos(true_pos_df, pred_df, true_df, cui=True):\n",
    "    # group similar entities in true pos df\n",
    "    temp = pd.DataFrame(true_pos_df.groupby(by=[\"Entity_label\", \"Entity_pred\"])[\"Start_pred\"].count()).reset_index()\n",
    "    grouped = pd.DataFrame(temp.groupby(by=[\"Entity_label\"])[\"Start_pred\"].sum()).sort_values(by=\"Start_pred\", ascending=False)\n",
    "    temp = temp.merge(grouped, on=\"Entity_label\")\n",
    "    temp.columns = [\"Entity_label\", \"Entity_pred\", \"Entity_pred count\", \"Entity_label count\"]\n",
    "    temp = temp.sort_values(by=[\"Entity_label count\", \"Entity_pred count\"], ascending=False)\n",
    "    true_pos_grouped = temp\n",
    "    \n",
    "    if cui:\n",
    "        columns = [\"Entity\", \"CUI\", \"TUI\"]\n",
    "    else:\n",
    "        columns = [\"Entity\"]\n",
    "    \n",
    "    # false positives - count overlap as match\n",
    "    temp = pred_df.merge(true_pos_df[[\"paper\", \"Start_pred\", \"End_pred\"]], left_on=[\"paper\", \"Start\", \"End\"], right_on=[\"paper\", \"Start_pred\", \"End_pred\"], how=\"outer\")\n",
    "    false_pos = temp[temp[\"Start_pred\"].isnull()].sort_values(by=[\"paper\", \"Entity\"])\n",
    "    false_pos_grouped = false_pos.groupby(by=columns)[\"Start\"].count().reset_index().sort_values(by=\"Start\", ascending=False).reset_index(drop=True)\n",
    "    false_pos_grouped = false_pos_grouped.rename(columns={\"Start\":\"count\"})\n",
    "    \n",
    "    # false negative - count overlap as match\n",
    "    temp = true_df.merge(true_pos_df[[\"paper\", \"Start_label\", \"End_label\"]], left_on=[\"paper\", \"Start\", \"End\"], right_on=[\"paper\", \"Start_label\", \"End_label\"], how=\"outer\").drop_duplicates([\"paper\", \"Start\", \"End\"])\n",
    "    false_neg = temp[temp[\"Start_label\"].isnull()].sort_values(by=[\"paper\", \"Entity\"])\n",
    "    false_neg_grouped = false_neg.groupby(by=columns)[\"Start\"].count().reset_index().sort_values(by=\"Start\", ascending=False).reset_index(drop=True)\n",
    "    false_neg_grouped = false_neg_grouped.rename(columns={\"Start\":\"count\"})\n",
    "    \n",
    "    return true_pos_grouped, false_pos_grouped, false_neg_grouped, false_pos, false_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard = pd.read_csv(\"gold-standard.csv\").drop([\"FWORD\", \"CODE\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUI</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C0004352</td>\n",
       "      <td>Autism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1510586</td>\n",
       "      <td>Autism Spectrum Disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0004352</td>\n",
       "      <td>Autistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C1510586</td>\n",
       "      <td>ASD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1510586</td>\n",
       "      <td>ASDs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C0236792</td>\n",
       "      <td>Asperger's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C0236792</td>\n",
       "      <td>Asperger Syndrome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CUI                      TEXT\n",
       "0  C0004352                    Autism\n",
       "1  C1510586  Autism Spectrum Disorder\n",
       "2  C0004352                  Autistic\n",
       "3  C1510586                       ASD\n",
       "4  C1510586                      ASDs\n",
       "5  C0236792                Asperger's\n",
       "6  C0236792         Asperger Syndrome"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels = pd.read_csv(\"new-labels.csv\")\n",
    "new_labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUI</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C0236792</td>\n",
       "      <td>Aspergers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1837434</td>\n",
       "      <td>ASPERGER SYNDROME, SUSCEPTIBILITY TO, 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1837646</td>\n",
       "      <td>ASPERGER SYNDROME, SUSCEPTIBILITY TO, 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C1837697</td>\n",
       "      <td>ASPERGER SYNDROME, SUSCEPTIBILITY TO, 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1864961</td>\n",
       "      <td>ASPERGER SYNDROME, SUSCEPTIBILITY TO, 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C1845334</td>\n",
       "      <td>ASPERGER SYNDROME, X-LINKED, SUSCEPTIBILITY TO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C1845341</td>\n",
       "      <td>ASPERGER SYNDROME, X-LINKED, SUSCEPTIBILITY TO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C3151708</td>\n",
       "      <td>ASPERGER SYNDROME, SUSCEPTIBILITY TO, X-LINKED 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C3151722</td>\n",
       "      <td>ASPERGER SYNDROME, SUSCEPTIBILITY TO, X-LINKED 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C0004352</td>\n",
       "      <td>Autistic Disorder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CUI                                               TEXT\n",
       "0  C0236792                                          Aspergers\n",
       "1  C1837434            ASPERGER SYNDROME, SUSCEPTIBILITY TO, 3\n",
       "2  C1837646            ASPERGER SYNDROME, SUSCEPTIBILITY TO, 1\n",
       "3  C1837697            ASPERGER SYNDROME, SUSCEPTIBILITY TO, 2\n",
       "4  C1864961            ASPERGER SYNDROME, SUSCEPTIBILITY TO, 4\n",
       "5  C1845334  ASPERGER SYNDROME, X-LINKED, SUSCEPTIBILITY TO...\n",
       "6  C1845341  ASPERGER SYNDROME, X-LINKED, SUSCEPTIBILITY TO...\n",
       "7  C3151708   ASPERGER SYNDROME, SUSCEPTIBILITY TO, X-LINKED 1\n",
       "8  C3151722   ASPERGER SYNDROME, SUSCEPTIBILITY TO, X-LINKED 2\n",
       "9  C0004352                                  Autistic Disorder"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_standard = gold_standard.append(new_labels)\n",
    "gold_standard.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'autistic children', 'did not initiate any interaction, verbal or nonverbal', \"likes to be around children, doesn't play with them\", 'no verbal communication', 'unusual hand movements', 'he only provides a fleeting eye contact', 'interacted with examiner in pleasant, but disengaged, manner', 'blowing on hand', 'moving his jaw, rubbing his hands together', 'he has no social interaction with other children', 'arm waving when excited', 'focused attention on wheels of car', 'repetitive spinning of whole body', 'making hee-haw sounds', 'no symbolic play', 'does not play with toys', 'runs on tip toes', 'hyper- or hypo-reactivity to sensory input or unusual interest in sensory aspects of environment; (such as apparent indifference to pain/heat/cold, adverse response to specific sounds or textures, excessive smelling or touching of objects, fascination with lights or spinning objects).', 'he did not initiate eye contact', 'solitary in his play', 'play on trampoline for hours', 'initiates very little in the way of play behavior', 'not interacting socially with others', 'very significant lack of interest in make-believe play', 'plays abnormally with toys', 'fans the pages of a book 20 minutes at a time', 'very little interaction with people around him', 'eye contact is observed 3 times during 45 minute interview', 'spontaneously engaging in other children', 'plays alone', 'did not intiate any eye contact', 'difficulty conveying wants & needs with language', 'shows interest in toys', 'play was simple and repetitive', 'watching wheels of trucks', 'occasionally echolalic', 'needing to get out of car in certain way, touching walls in rooms', 'walking on toes', 'craving unusual or nofood items', 'atypical pragmatic language skills', 'speech and language delay', 'he can be totally absorbed in watching television', 'has to be drawn into interactions with other children', 'ritualistic behaviour', 'did show evidence of initiating an interaction', 'repetitive hand washing', 'runs in circles', 'self destructive behavior', 'prefers parallel play', 'insistence on sameness', 'no friends', 'delayed language', 'likes to play by himself', 'lacks spontaneous make believe play', 'withdrawal from social contact', 'repetitive finger movements', 'difficulty understanding appropriate use of pronouns and speaking', 'speech delay', 'interest in peers', 'zero interaction with people', 'abnormality of toe walking', 'pervasive and specific developmental disorders (f80-f89)', 'observes other children rather than join in', 'shows interest in peer relations', 'abnormal response to sound', 'perseverates on various topics', 'autism', 'avoided direct gaze', 'interested in interacting with other children', 'inappropriate giggling', 'limited interaction with other individuals', 'stereotypic play with wheels', 'limited expressive language', 'receptive & expressive language difficulties', 'body rocking', 'imitation (learning)', 'preoccupation with narrowly focused interest', 'interest with mechanical devices', 'autism spectrum disorder', 'repetitively labeling objects', 'poor eye contact', 'difficulty playing with peers, particularly in terms of both initiating and maintaining contact', 'does not establish eye contact', \"doesn't have much interaction with children\", 'obsessive rituals before going to bed, opening cupboards', 'toys used in purposeful manner', 'playing with cage / netting over and over', 'hand clapping', 'inflexibility with regard to routines', 'is interactive', 'made intermittent eye contact', 'is seeking social interactions', 'language delayed', 'play is limited to a few preferred items', 'restricted interest in play', 'speech delayed', 'picking at objects until disassembled', 'he cannot tolerate changes in his routine', 'pica', 'some echolalic tendencies were heard', 'infantile autism, current or active state', 'facial expressions constricted, and eye contact', 'plugging ear with thumb', 'she has no words', 'speech echolalic', 'number of rituals', 'asperger syndrome, x-linked, susceptibility to, 2 (finding)', 'hand flapping when agitated', 'withdrawal; social', 'refusal to interact with other children', 'speech, aprosodic', 'interacted with toys only by naming them', 'he avoided eye contact with examiner', 'licking carpet during exam', 'delayed or lack of receptive language', 'repetitive rocking movements', 'will not play with others', 'not spontaneously engaging in other children', 'imaginary play minimal', 'preoccupied with living things', 'no indication of creative or representational play', 'no spontaneous social interaction', 'did initiate contact', 'picking or pulling hair out', 'avoids play with his peers', 'his understanding of pragmatic language appears to be quite limited', 'atypical pervasive developmental disorder', 'does not make good eye contact', 'initiating social interactions', 'he will resist being held, hugged or shown physical affection', 'setting up, knocking down objects repeatedly', 'eye contact is present about 1/2 of time', 'expressionless face', 'no attempt to interact with examiner', 'limited speech', 'lack of \"connecting\"', 'still not able to initiate a social interaction', 'finger flicking in front of face', 'echo speech', 'could not be engaged in social interaction', 'self injurious behavior', 'no interest in peers', \"strange, unusual 'oowing' noise\", 'never playing with other children', 'general disregard in social interaction', 'eye contact mildly diminished', 'auditory self-stimulation - singing, vocalizations', 'head banging on floor repeatedly', 'eye contact never established', 'play with toys is not very functional', 'intermittent eye contact', 'autism spectrum disorder (less common)', 'stacking books, touching rituals', 'appears not to pick up on conversational cues well', \"upset if meals don't occur at exactly the same time\", 'delays speech', 'disassemble', 'eats abnormal objects', 'makes very little eye contact', 'eye contact was marginal, usually fleeting, typified by lack of social acknowledgment', 'initiates social behavior with family or individuals', 'impairment of peer relationships', 'autistic-like features', 'interacted with examiner in quiet, withdrawn manner', 'talks / answers himself in a different voice', 'limited eye contact', 'hyper-reactivity to sensory input', 'other pervasive developmental disorders', 'usually plays alone', 'sing-songy sounds', 'she did not engage in my obvious pretend play', 'litte in the way of social interactive play', 'hand sucking', 'no pragmatic language', 'socially isolated', 'deliberate self harm', 'asperger syndrome, susceptibility to, 2', 'engages in creative play', 'enjoys spinning', 'head shaking', 'no effort to initiate play with other children', 'no indication of affect in his face', 'social interaction limited, particularly in group setting', 'making voices', 'repetitive head banging', 'twirling and spinning', 'repetitious, recitation-like quality', 'he has a fascination with objects', 'has manifested stereotypic - like movements', 'repeating words', 'repeating sounds in perseverative way', 'no imaginative play', 'play is limited and quite stereotypic', 'has few toys', \"watches children, but doesn't participate\", 'palilalia', 'other specified pervasive developmental disorders', 'eye contact very poor', 'likes to watch wheels spin', 'obsessed with tire swings, color yellow (stacking only yellow blocks, using only yellow crayons)', 'pretty much a loner', 'does not engage in neaningful reciprocal social interactions', 'hair flicking', 'makes little eye contact', 'autistic thinking', 'very limited eye contact', 'he only plays by himself', 'behavioral changes consistent with an autistic disorder', 'language has always been delayed', 'he does not talk', 'no converstional speech', 'he engages in little cooperative play', 'mimics sounds', 'autistic-like behavior', 'echophrasia', 'severe delay in socializations', 'stereotypic finger movements', 'needs lots of structure and consistency', 'asperger syndrome, susceptibility to, 1', 'perseverates on a single topic', 'rapid finger movements', 'picking at wallpaper until whole room is free of wallpaper', 'only rarely likes to play with other children', 'likes to spin in circles', 'flapping movements of hands', 'non-communicating child', 'difficulty establishing close relationships', 'continues stereotypical play', 'autistic disorder of childhood onset with residual state', 'started using toys', \"distress if parents drive different way to grandmother's house\", 'non-verbal', 'inability to play with other children', 'easily upset by change in routines', 'does not interact with other children', 'his favorite activity is to walk around waving coat hangers in front of himself', 'does not use pronouns', 'pervasive developmental disorders nec', 'uninterested in interacting with family or other children', 'no eye contact', 'did not actively engage with this examiner', 'becomes upset if parents fail to follow specific car route', 'asperger syndrome, susceptibility to, 3', 'reaction, echo', 'does not interact much with other children', 'did not show any evidence of initiating an interaction', 'spontaneous social interaction', 'plays with toys appropriately', 'plays randomly with toys', 'requires that his brother say exact same words each night during the bedtime ritual', 'some stereotypic hand movements were observed', 'atypical stereotypic motor mannerisms', 'tactile defensiveness (finger paints, playdough, shaving cream, etc.)', 'uses people as objects', 'delayed echolalia', 'imitation', 'difficulty using non-verbal communication', 'parallel play', 'preoccupation with fire', 'likes to have something in hand which he tends to fondle and tear up', 'does not play with manipulative toys', 'pervasive developmental disorder (cvs+)', 'nonverbal', 'has made few if any friends', 'has about five words', 'picking at her nose for hours at a time', 'speech limited', 'mostly watches other children', 'likes to tie things together', 'becomes very upset with change to his routine', 'always been a loner', 'eye contact is present only sporadically; exhibits social smile only sporadically', 'has great difficulty interacting with other children', 'delay language', 'eye contact moderately reduced', 'rote phrases', 'plays with toys', 'certainly does seek out attention', 'autism spectrum disorders', 'manic - pacing', 'does not initiate any social behavior with family or individuals', 'walks in straight and specific patterns', 'asperger syndrome, x-linked, susceptibility to, 1 (disorder)', 'very little eye contact', 'cannot be engaged', 'never looks you in the eye', 'if the routines of her life are broken, she becomes quite angry', 'number of instances of idiosyncratic movements with his hands', 'psychopathy; autistic', 'solitary play', 'has friends, but is socially inept', 'delayed articulation', 'often perseverated with a word for days at a time', 'much of his speech is repetitive', 'she will stare at books or wrap string around fingers for hours', 'play with toys was without imaginative control', 'little eye contact', 'autistic features (rare)', 'no intelligible speech', 'trouble making eye contact', 'he is not involved in interactive play', 'likes to line objects up', 'play was simple and repetitive, indeed perseverative', 'deliberate self-harm', 'interacts with peers', 'uses hand as tool', 'repetitive spinning of objects', 'delayed; speech', 'tipping a cup of water into the sink over and over', 'does not show any interest in peer relations', 'peer interactions are directed by self', 'not really connected socially', 'peculiar preoccupation with horizontal and vertical planes', 'echo reaction', 'likes to watch the mixer', 'atypical prosody', 'interested in interacting with family or other children', 'language continues to be quite stereotypical and quite echolalic', 'string 2 to 3 words together at a time', 'waving things in front of his face', 'lip smacking', 'little social interaction with others', 'significantly delayed speech and language development', 'pacing up and down', 'repetitively lining up toys', 'language delays', 'did not establish good eye contact', 'plays for long periods by himself', 'consistent lack of social reciprocity', 'very focused and intense on single activities / objects to the point of tuning everyone out', 'avoidant of mutual play', 'lack of expressive language', 'prefers to look at people out of corner of eye', 'unresponsive to requests', 'slapping himself', 'sits on bed and rocks', 'zero interactive play', 'peculiar, avoidant, disengaged', 'repetitious behavior', 'speech delays', 'cannot initiate conversation', 'never developed normal speech or language', 'asd', 'social use of language was below expectations', 'social communication disorder', 'preoccupation with parts of objects', 'obsessed with elevators, writing, and drawing', 'preoccupation with bouncing ball', 'playing excessively with certain objects', 'voluntary interaction with therapist', 'eye contact only fleetingly achieved', 'eye contact non-existent', 'she is not interactive', 'motor jerks (stereotypical or repetitive - not a tic)', 'cry as hypersensitive to stimulus', 'excessive focus on cars and trains', 'fixated on horses', 'raising one of his arms', 'no functional language', 'head banging', 'insists on sameness', 'interacts with other children', 'facial expression is lacking for any social interchange', 'turning water and light switches on and off', 'parents describe him as a loner', 'spins around in circles', 'some finger waving by eye', 'stereotypical utterances', 'assiduously avoided any eye contact', 'autistic', 'does not like to be touched', 'not engaging socially', 'hopping', 'marked impairment in language', 'repeatedly turning over hands', 'flat facial expression', 'does not respond to pain appropriately', 'rarely maintained eye contact', 'occasional brief eye contact', 'asperger syndrome, susceptibility to, x-linked 1', 'does not play with toys appropriately', 'echolalia tendencies heard', 'he did not provide any eye contact', 'copying', 'does not engage or play with other children normally', 'fascinated by parts of objects', \"enamored of television country music station, will watch for 2-3 hours (note: issue is not time it's the 'enamored/perseverative' issue)\", 'aspergers', 'did not sustain eye contact', 'limited in interactions with other children', 'aprosody', 'he can become fixated on an activity to the point that it is impossible to redirect him', 'play is extremely limited', 'autistic features (if left untreated)', 'did not participate in any turn taking activities', 'language milestones quite delayed', 'has social interaction with other children', 'autistic disorder', 'unusual sounds such as squealing or guttural sounds', 'interacted with toys in addition to naming them', 'relatively poor social interaction', 'looking through you, content to be in her own little world', 'usually resists undertaking any activity', 'general self stimulation', 'impaired social interactions', 'delayed; articulation', 'repeating non-sense sounds', 'very little verbal response', 'inspected toys and played with toys', 'asperger syndrome, susceptibility to, 4', 'unable to sustain conversation', 'spinning', 'does not engage in much imaginative play', 'repetitively playing with water', 'easily upset when routines are broken', \"asperger's\", 'she certainly does not seek out attention', 'self-injurious behavior', 'significant lack in communication skills', 'pervasive development disorder', 'prosodic contour odd, generally high in pitch', 'engaging in some repetitive behavior', 'atypical autism', 'eye contact of marginal quality', 'not interested in interacting with other children', 'his interactions with others tends to be quite limited', 'rigidity of routines', 'atypical child with whom to interact', 'able to initiate a social interaction', 'preoccupied with medical instruments', 'moving foot in stereotypic manner', 'makes scarce eye contact', 'hand flapping when excited', 'stereotypical movements', 'self-destructive behavior', 'overall speech is delayed', 'avoiding any eye contact', 'continues to manifest atypicalities in interactionaly style and language pattern', 'isolative behavior', 'playing for long periods with trains, arranging and rearranging in different patterns', 'hand / finger wiggling', 'does not play with others', 'loves to tap finger on table', 'lost any interest in playing interactive games', 'desire for sameness, some intolerance for change', 'language consists of \"barney\" and \" no\"', 'plays with manipulative toys', 'seems to parrot or mimic what is said', 'will manipulate things, take things apart', 'socially, he isolates himself', 'likes repetition and routine', 'difficult to obtain or maintain eye contact', 'does not have good peer relationships', 'plays by himself', 'he arranges his cars over and over again in lines', 'repeated certain words over and over', 'diminished eye contact with examiner', 'social interaction disorder', 'picks at his skin', 'repetitive mannerisms described', 'fascinated by mechanical objects', 'restricted ability to initiate contact', 'moderate impairment in pragmatic language', 'unable to maintain conversation', 'initiates social exchanges', 'great deal of echolalia', 'autism or autistic-like condition', 'not engaging in any specific play activity', 'fascination with water', 'he likes to spin and twirl', 'no spontaneous speech', 'engaging in specific play activity', 'hand licking', 'mask-like facial expression', 'eye contact difficult to obtain, tended to be fleeting', 'difficulty with social interaction', 'unintelligible speech', 'expressive language, often echolalic-like', 'repetitive speech', 'does not interact with peers', 'delayed speech and language', 'difficulty sustaining conversation', 'picks at nose stereotypically with thumb and index finger', 'likes to watch objects spin', 'obsesses on bendable objects', 'fascination with engines and keys', 'speaking in third person', 'fascinated by the sound when computer is turned on; when air is squeezed out of bottle', 'will repeatedly toss blocks to floor and watch them scatter', 'interaction with people', 'preoccupation with cutting up paper into small pieces', 'imitative play', 'initiate any interaction, verbal or nonverbal', 'is in his own little world', 'language skills limited', 'language delay', 'decreased interaction with other children', 'shows a need for routine', 'reciprocity', 'preoccupied with wires and cords', 'narrowed involvement with play activities', 'clapping hands on thighs in a very rapid fashion', 'some stereotypic movements observed', 'sometimes making poor eye contact', 'some evidence of echolalia', 'lack of facial expression', 'enjoys watching spinning fans', \"doesn't play with anyone\", 'minimal eye contact', 'does not seek out contact with other children', \"doesn't socially interact\", 'stereotypic vocalization', 'will not interact in a socially meaningful manner', 'inspected toys but did not play with toys', 'lining up pictures of presidents', 'hand shaking', 'finger flicking', 'likes to have matters on things the same', 'memorizes lines and dialogue from movies and stories', 'impaired social interaction', 'tolerant of cold', 'preoccupation with television', 'social interaction clearly disturbed', 'problems with reciprocal social interactions', 'fascination with spinning fans', 'abnormal non-verbal communcation', 'very preoccupied with cartoons', 'pacing in circles', 'non-verbal social skills are poor', 'does not mix with other children', 'marked impairment of interperosnal relations', 'initiates play behavior', 'did not initiate any contact', 'not playing interactively', 'language non-existent', 'trains', 'shaking of hands, shuddering', 'no systematic, interactive non-verbal communication', 'stopped using toys', 'difficulty with concrete speech and language', 'prefers isolated/autonomous activities; no close friends', 'engaging in some repetitive behaviors', 'seems to want to be in the same room as other children, but is not actively involved in their activity', 'semantic-pragmatic impairment', 'minimal interaction with examiner', 'hand waving', 'asperger syndrome, susceptibility to, x-linked 2', 'fast forward and rewinds videos to which he is deeply attached and sleeps with', 'uses only single words', 'autistic features', 'differently interacting with other children', 'behaves as if deaf', 'delayed speech', 'often repeats phrases from videos', 'has some language difficulties as well as flipping words in sentences and difficulty with pronunciation', 'prefers to play alone', 'indifference to pain', 'little evidence of pretend / creative play', 'repetitive flexion / extension movements of his fingers', 'poor social skills or interest in social interaction with peers', 'plays with people as objects', 'inappropriate eating behavior', 'aprosodia', 'unable to speak in sentences', 'unusual noises', 'tends to relate to objects as their parts as opposed to their whole', 'arm biting', 'no imitative play', 'hair pulling', 'hypersensitive to stimuli', 'self-abusive behavior', 'high pitched quality of speech', 'staring into mirror, lights', 'eye contact very difficult', 'body posturing', 'bites fingers until red and swollen', 'unusual posturing; jumping while holding crotch', 'peer interactions are directed by other children', 'only plays by lining things up', 'turning light switch on and off repeatedly', 'communicates by grunting, screaming, crying', 'echolalic tendencies in his toddler years', 'repetitive clapping', 'difficult to maintain eye contact', 'difficulty with changes in routine', 'difficulty answering questions', 'verbal repetition', 'only a few toys with which he will play', 'hand movements', 'obsessively focused on books, videos, and pictures', 'quite marked resistance to change', 'loves to spin in circles', 'makes very poor eye contact', 'profound language delay', 'self-stimulatory behaviors including spinning', 'spinning wheels of toys', 'lining up objects', 'ritualistic behavior', 'hand flapping', 'some self-stimulating verbal activity - stereotype behavior of naming letters & some numbers with articulation', 'turning / twirling objects', 'seeks out contact with other children', 'grunting', 'spinning wheels repetitively', 'social interaction very limited', 'fearful to try new activities', 'delayed language abilities', 'perseverative play', 'eye contact fair', 'social withdrawal', 'mental retardation; autistic features', 'shows poor eye contact', 'rarely establishes eye contact', 'history of failure to develop peer relationships', 'self-directed behavior', 'resistant to change', 'disassembly', 'he seems very fixated on doing things in his own order', 'unable to ask for things using words', 'very little eye contact with others', 'likes to do puzzles over and over again', 'preoccupation with water', 'fascinated by anything with letters', 'rocking', 'does not use body posture to communicate', 'sense of disconnectedness was frequently present', 'talking to self', 'repetitive activities such as counting objects', 'speech tone high-pitched', 'fills and empties bank with coins repeatedly', 'preoccupied with touching things', 'arranging objects in straight line', 'rarely makes eye contact', 'toe-walking', 'lining up toys', 'no words', 'decreased social interactions', 'toe-walking gait', 'some echolalia', 'pacing the floor', 'resists being held, hugged or shown physical affection', 'does not play with toys in the fashion intended', 'lacks socialization skills with other children', 'somewhat withdrawn', 'eye contact abnormal', 'his play is described as intense', 'tends to twirl ropes or strings throughout the house repetitively', 'finger-play behavior', 'asperger syndrome', 'difficult to engage in spontaneous conversation', 'copying (learning)', 'toe walking', 'autistic behaviour', 'idiosyncratic movements, walking fingers across table', 'does not play with other children', 'ringing of hands', 'child development disorders, pervasive', 'tolerant of heat', 'grasshoppers', 'socially, he is a loner', 'does not usually play with other children', 'eye contact was variable', 'memorizing telephone numbers', 'difficulty with conversational skills', 'socially disconnetcted', 'adherence to routine', 'engages in neaningful reciprocal social interactions', 'spends much time filtering corn through his hands', 'perseverative at watching videos and other activities', \"won't play with other kids\", 'repetition, nos', 'preoccupation of patterns of doing things', 'initiates little in the way of contact with friends', 'spoke in broken sentences (\"me do\")', 'slapping hands under chin', 'self-harm, deliberate', 'limited vocabulary', 'severe speech/language retardation', 'no social responsiveness', 'delayed; language', 'unintelligible vocalizations', 'avoided eye contact', 'hitting jaw with hand to click teeth', 'finger wiggling', 'sensory intolerance', 'repeats / imitates almost all words said to him', 'totally unresponsive to efforts to engage him interpersonally', 'lack of eye contact', 'not socially connected to parents', \"played with toys by mother's side\", 'pays little attention ot family or examiner', 'prefers a consistent routine; cries if changes are made', 'no meaningful language', 'perseverative behaviors', 'she is not seeking social interactions', 'shows little interest in toys', 'memorization of lengthy book passages', 'difficulty making conversation', 'need to touch walls in certain places at certain times of the day', 'moderate eye contact', 'routines very important; if altered he becomes agitated', 'has watched particular video 100-200 times', 'some self-stimulatory behavior', 'speech, echo', 'decreased eye contact', 'no interest in playing with others', 'uninterested in playing with other children', 'no evidence of creative play', 'suggests imitation', 'poverty of facial expression', 'starring blankly into space', 'very little in the way of social resposiveness or rapport', 'impairment in eye-to-eye gaze', 'limited social interaction', 'eye contact less than optimal', 'no eye contact with examiner', 'no friends in his peer group', 'fascinated by spinning objects', 'makes effort at maintaining routine', 'looked at me only fleetingly', 'meaningful interaction with toys', 'loner', 'limited play', 'toys not used in any purposeful manner', 'fascinated with dolls that have eyes that open and close', 'stereotyped, repetitive behavior', 'slow to talk', 'autistic spectrum disorder with isolated skills', 'preoccupation with trash, bathrooms, memorizing birthdays', 'inconsistent eye contact', 'use of jargon', 'eye contact fleeting', 'learning imitation', 'autistic disorder of childhood onset with full syndrome', 'bothered by loud volume on radio', 'abnormal craving', 'mental retardation with language impairment and autistic features, 390-kb de', \"doesn't play games with family members or friends\", 'practices spinning, twirling his body', 'initiates only limited social exchanges', 'echolalia', 'ritualized behaviors', 'inflexible adherence to routines or rituals', 'he does not spontaneously seek to share interests or achievements with other people', \"uses examiner's hand as tool\", 'communication verbal', 'enters into interactions with other children', 'twisting his nipples, repetitively striking his finger against table', 'not attending to social approaches from adults', 'agrees to interact with other children', 'angry in response to change in routine', 'has had profound language delay', 'she does engage in interactive play with other children easily', 'verbal communication', 'does not appropriately respond verbally', 'parallel play only', 'mostly ignored me', 'does not use gestures to communciate', 'engages in no definite social play with age mates', 'lining up, grouping toys', 'idiosyncratic language abnormalities', 'minimal interaction with peers', 'arm flapping', 'loves to spin', 'preoccupation with spinning wheels', 'withdrawn, passive behavior', 'fails to use language as a form of social interaction', 'body-rocking', 'asds', 'eye contact is virtually nill', 'pounds head', \"repetitive stereotypical phrases such as 'you bet'\", 'compulsion for jigsaw puzzles', 'high-pitched and repetitive vocalizations', 'interacting socially with others', 'does not engage in reciprocal play', 'lining up cars', 'c0150080', 'clear-cut abnormalites in area of social interaction', 'content of speech seems lacking much of the time', 'profound fascination for water, balloon', 'refused efforts to interact', 'no meaningful interaction with toys', 'rarely says any words', 'aprosodic speech', 'repeatedly putting puzzles together', 'self-directed', 'pacing', 'he has very little in the way of sentences', 'engages in much imaginative play', 'most of time he plays by himself', 'repetitive, stereotypic utterances', 'enjoys spinning wheels of cars', 'initiates contact with friends', 'if this activity is disrupted (lining up objects), he exhibits anger', 'socially interacts', 'fixation on certain objects and colors', 'hitting himself on head', 'disregards all atempts to interact with him', 'opens and closes toy car doors and looks inside over and over', 'answers virtually no questions in a meaningful manner', 'no clear history of interactive play with other children', 'stereotyped phrases', 'imaginary play great', 'carefully scrutinizes parts of toys', 'very demanding of sameness', 'no voluntary interaction with therapist', 'lines up objects', 'social disconnectedness', 'nothing to suggest imitation', 'trouble playing with toys', 'does spontaneously seek to share interests or achievements with other people', 'lack of communicator initiative & responsiveness', 'restrictive behavior, interests, and activities', 'smells everything', 'likes playing with boxes, will line them up in a stereotypical manner and will walk around and look at them', 'delayed speech and language development', 'occasions in which he will demonstrate repetitive movements', 'ignoring questions', 'stereotypic movements', 'does not initiate conversation', 'very poor eye contact', 'tends to communicate by grunting & screaming', 'tapping finger and thumb together', 'stereotypical motor behavior', 'speech often shrill / high pitched when excited', 'changes can be very upsetting to her', 'refusing to eat food if not cut in a certain way', 'very limited in the quality of social interaction with other children', 'lacks social skills (poor, impaired, limited)'}\n"
     ]
    }
   ],
   "source": [
    "gold_standard[\"TEXT\"] = gold_standard[\"TEXT\"].str.strip().str.lower()\n",
    "autism_terms = set(gold_standard[\"TEXT\"])\n",
    "print(autism_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spacy token matcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(text) for text in autism_terms]\n",
    "matcher.add(\"AutismTerms\", None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold standard CUI to TUI\n",
    "gold_standard_cui_to_tui = pd.read_csv(\"tui_list_gold_standard.txt\", sep=\"\\t\", index_col=0, header=None).reset_index()\n",
    "gold_standard_cui_to_tui.columns = [\"CUI\", \"TUI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "830"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(autism_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaMap results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metamap input and output\n",
    "METAMAP_DIRECTORY = \"metamap\"\n",
    "\n",
    "INPUT_DIRECTORY_FULL_TEXT = \"pubmed_all_fulltext_input\"\n",
    "INPUT_DIRECTORY_ABSTRACT = \"pubmed_abstracts_24860\"\n",
    "\n",
    "METAMAP_OUTPUT_DIRECTORY_FULL_TEXT = os.path.join(METAMAP_DIRECTORY, \"metamap_asciiignorexmlparsed\") \n",
    "METAMAP_OUTPUT_DIRECTORY_ABSTRACT = os.path.join(METAMAP_DIRECTORY, \"metamap_output_abstract_ignore_parsed\") \n",
    "\n",
    "METAMAP_RESULTS_DIRECTORY_FULL_TEXT = os.path.join(METAMAP_DIRECTORY, \"metamap_results_full_text\")\n",
    "METAMAP_RESULTS_DIRECTORY_ABSTRACT = os.path.join(METAMAP_DIRECTORY, \"metamap_results_abstract\")\n",
    "\n",
    "# choose whether to use full-text or abstract - MODIFY HERE\n",
    "ABSTRACT = False\n",
    "\n",
    "if ABSTRACT:\n",
    "    METAMAP_OUTPUT_DIRECTORY = METAMAP_OUTPUT_DIRECTORY_ABSTRACT\n",
    "    METAMAP_RESULTS_DIRECTORY = METAMAP_RESULTS_DIRECTORY_ABSTRACT \n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_ABSTRACT\n",
    "    METAMAP_FULL_TEXT_DIRECTORY = os.path.join(METAMAP_DIRECTORY, \"metamap_abstract\")\n",
    "else:\n",
    "    METAMAP_OUTPUT_DIRECTORY = METAMAP_OUTPUT_DIRECTORY_FULL_TEXT\n",
    "    METAMAP_RESULTS_DIRECTORY = METAMAP_RESULTS_DIRECTORY_FULL_TEXT\n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_FULL_TEXT\n",
    "    METAMAP_FULL_TEXT_DIRECTORY = os.path.join(METAMAP_DIRECTORY, \"metamap_full_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamap_files = os.listdir(METAMAP_OUTPUT_DIRECTORY)\n",
    "if '.DS_Store'in metamap_files:\n",
    "    metamap_files.remove('.DS_Store')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = os.listdir(INPUT_DIRECTORY)\n",
    "if '.DS_Store'in input_files:\n",
    "    input_files.remove('.DS_Store')\n",
    "    \n",
    "input_files = [f.replace(\".txt\", \"\") for f in input_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamap_papers = set([f.split(\"_\")[0] for f in metamap_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamap_files = sorted(metamap_files, key = lambda x: (x.split(\"_\")[0], int(x.split(\"_\")[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in metamap_files if \"_\" in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"id\tMappingScore\tCandidateCUI\tCandidateMatched\tSemType\tStartPos\tLength\tNegated\tCandidateScore\tMatchedWords\\n\"\n",
    "columns = [\"id\", \"MappingScore\", \"CandidateCUI\", \"CandidateMatched\", \"SemType\", \"StartPos\", \"Length\", \"Negated\", \"CandidateScore\", \"MatchedWords\"]\n",
    "true_temp_df = pd.DataFrame()\n",
    "pred_temp_df = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "papers_analyzed = []\n",
    "full_text = \"\"\n",
    "for filename in metamap_files:\n",
    "    \n",
    "    if \"PMC6061181\" in filename: # skip this paper\n",
    "        continue\n",
    "    \n",
    "    if \".DS_Store\" in filename:\n",
    "        continue\n",
    "    \n",
    "    if \"_\" in filename:\n",
    "    \n",
    "        paper = filename.split(\"_\")[0]\n",
    "        \n",
    "        # new paper\n",
    "        if paper not in papers_analyzed:\n",
    "            \n",
    "            if len(papers_analyzed) > 0:\n",
    "                \n",
    "                with open(os.path.join(METAMAP_FULL_TEXT_DIRECTORY, papers_analyzed[-1] + \".txt\"), \"w\") as f:\n",
    "                    f.write(full_text)\n",
    "                \n",
    "                # analyze previous paper\n",
    "                doc = nlp(full_text)\n",
    "                matches = matcher(doc)\n",
    "                spans = []\n",
    "\n",
    "                for match_id, start, end in matches:\n",
    "                    span = doc[start:end]\n",
    "                    spans.append(span)\n",
    "\n",
    "                filtered = spacy.util.filter_spans(spans)\n",
    "\n",
    "                for span in filtered:\n",
    "                    ent = span.text.lower().strip()\n",
    "                    #temp = pd.DataFrame({\"Entity\": ent, \"paper\": filename, \"Start\": span.start_char, \"End\":span.end_char, \"Sentence\":span.sent.text}, index=[j])\n",
    "                    temp = pd.DataFrame({\"Entity\": span.text, \"Entity_lower\": ent, \"paper\": papers_analyzed[-1], \"Start\": span.start_char, \"End\":span.end_char, \"Sentence\": span.sent.text}, index=[j])\n",
    "                    true_temp_df = true_temp_df.append(temp)\n",
    "                    j = j + 1\n",
    "            \n",
    "            if len(papers_analyzed) % 100 == 0:\n",
    "                print(len(papers_analyzed), paper)\n",
    "                \n",
    "            full_text = \"\"\n",
    "            papers_analyzed.append(paper)\n",
    "            \n",
    "            \n",
    "        with open(os.path.join(METAMAP_OUTPUT_DIRECTORY, filename), \"r\") as f:\n",
    "            data = f.read()\n",
    "            \n",
    "        if header not in data:\n",
    "            print(filename, \"has no header\")\n",
    "            \n",
    "        splits = data.split(header)\n",
    "        \n",
    "        # this part contains the pmid and utterances\n",
    "        info = splits[0].split(\"\\n\")\n",
    "        pmid = \"\"\n",
    "        utterance = False\n",
    "        start_idx = len(full_text)\n",
    "        \n",
    "        for line in info:\n",
    "            if \"PMID: \" in line:\n",
    "                pmid_found = line.replace(\"PMID: \", \"\")\n",
    "                pmid_found = pmid_found.split(\"_\")[0]\n",
    "                \n",
    "                # check if pmid matches paper\n",
    "                if pmid_found != paper:\n",
    "                    raise Exception(\"PMID doesn't match paper:\", line)\n",
    "                else:\n",
    "                    pmid = pmid_found\n",
    "                \n",
    "            if utterance:\n",
    "                full_text = full_text + line\n",
    "            \n",
    "            if \"UttText:\" in line:\n",
    "                utterance = True\n",
    "            else:\n",
    "                utterance = False\n",
    "            \n",
    "        full_text = full_text + \" \"\n",
    "        \n",
    "        # no terms detected\n",
    "        if len(splits) < 2:\n",
    "            i = i + 1\n",
    "            continue\n",
    "        \n",
    "        temp = pd.read_csv(StringIO(splits[1]), sep=\"\\t\", header=None)\n",
    "        temp.columns = columns\n",
    "        \n",
    "        temp[\"MatchedPhrase\"] = temp[\"MatchedWords\"].apply(lambda x: \" \".join(str(x).split(\",\")))\n",
    "        temp[\"paper\"] = paper\n",
    "        temp[\"paper_part\"] = filename\n",
    "        temp = temp.rename(columns={\"StartPos\": \"Start\", \"CandidateCUI\":\"CUI\"})\n",
    "        temp[\"Start\"] = temp[\"Start\"] + start_idx\n",
    "        temp[\"End\"] = temp[\"Start\"] + temp[\"Length\"]\n",
    "        temp = temp.drop([\"id\", \"Length\", \"MappingScore\"], axis=1)\n",
    "        temp[\"Entity_matched\"] = temp.apply(lambda row: full_text[row['Start']:row['End']], axis=1)\n",
    "        # only use row with highest CandidateScore for given StartPos and Length\n",
    "        temp = temp.sort_values('CandidateScore', ascending=True).drop_duplicates(['Start','End']).sort_values(by=\"Start\").reset_index(drop=True)\n",
    "        temp = temp.rename(columns={'CandidateMatched':'Entity'})\n",
    "        pred_temp_df = pred_temp_df.append(temp)\n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "        \n",
    "# analyze previous paper\n",
    "with open(os.path.join(METAMAP_FULL_TEXT_DIRECTORY, papers_analyzed[-1] + \".txt\"), \"w\") as f:\n",
    "    f.write(full_text)\n",
    "\n",
    "doc = nlp(full_text)\n",
    "matches = matcher(doc)\n",
    "spans = []\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    spans.append(span)\n",
    "\n",
    "filtered = spacy.util.filter_spans(spans)\n",
    "\n",
    "for span in filtered:\n",
    "    ent = span.text.lower().strip()\n",
    "    #temp = pd.DataFrame({\"Entity\": ent, \"paper\": filename, \"Start\": span.start_char, \"End\":span.end_char, \"Sentence\":span.sent.text}, index=[j])\n",
    "    temp = pd.DataFrame({\"Entity\": span.text, \"Entity_lower\": ent, \"paper\": papers_analyzed[-1], \"Start\": span.start_char, \"End\":span.end_char, \"Sentence\": span.sent.text}, index=[j])\n",
    "    true_temp_df = true_temp_df.append(temp)\n",
    "    j = j + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MetaMap number of predicted entities =\", len(pred_temp_df))\n",
    "pred_temp_df.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, \"metamap_pred_temp_df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MetaMap number of labelled entities = \", len(true_temp_df))\n",
    "true_temp_df.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, \"metamap_true_temp_df.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start here if you want to load metamap results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metamap input and output\n",
    "METAMAP_DIRECTORY = \"metamap\"\n",
    "\n",
    "INPUT_DIRECTORY_FULL_TEXT = \"pubmed_all_fulltext_input\"\n",
    "INPUT_DIRECTORY_ABSTRACT = \"pubmed_abstracts_24860\"\n",
    "\n",
    "METAMAP_OUTPUT_DIRECTORY_FULL_TEXT = os.path.join(METAMAP_DIRECTORY, \"metamap_output_full_text\") \n",
    "METAMAP_OUTPUT_DIRECTORY_ABSTRACT = os.path.join(METAMAP_DIRECTORY, \"metamap_output_abstract\") \n",
    "\n",
    "METAMAP_RESULTS_DIRECTORY_FULL_TEXT = os.path.join(METAMAP_DIRECTORY, \"metamap_results_full_text\")\n",
    "METAMAP_RESULTS_DIRECTORY_ABSTRACT = os.path.join(METAMAP_DIRECTORY, \"metamap_results_abstract\")\n",
    "\n",
    "# choose whether to use full-text or abstract - MODIFY HERE\n",
    "ABSTRACT = True\n",
    "\n",
    "if ABSTRACT:\n",
    "    METAMAP_OUTPUT_DIRECTORY = METAMAP_OUTPUT_DIRECTORY_ABSTRACT\n",
    "    METAMAP_RESULTS_DIRECTORY = METAMAP_RESULTS_DIRECTORY_ABSTRACT \n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_ABSTRACT\n",
    "else:\n",
    "    METAMAP_OUTPUT_DIRECTORY = METAMAP_OUTPUT_DIRECTORY_FULL_TEXT\n",
    "    METAMAP_RESULTS_DIRECTORY = METAMAP_RESULTS_DIRECTORY_FULL_TEXT\n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_FULL_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ABSTRACT:\n",
    "    true_temp_df = pd.read_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, \"metamap_true_temp_df.csv\"), index_col=0)\n",
    "    pred_temp_df = pd.read_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, \"metamap_pred_temp_df.csv\"), index_col=0)\n",
    "    \n",
    "else:\n",
    "    true_temp_df = pd.read_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, \"metamap_true_temp_df.csv\"), index_col=0)\n",
    "    pred_temp_df = pd.read_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, \"metamap_pred_temp_df.csv\"), index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(true_temp_df[\"Entity_lower\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_temp_df = pred_temp_df.rename(columns={\"SemType\":\"TUI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distinct true entities detected:\", len(set(true_temp_df[\"Entity\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER:\n",
    "    # filter predicted terms for relevant semtypes\n",
    "    relevant_semtypes = [\"fndg\", \"mobd\"]\n",
    "    pred_temp_df_filtered = pred_temp_df[(pred_temp_df[\"TUI\"].isin(relevant_semtypes)) | (pred_temp_df[\"Entity\"].str.contains(\"ASD\"))]\n",
    "    pred_df_metamap = filter_pred(pred_temp_df_filtered, remove_non_asd=True) # use this to only use relevant semtypes\n",
    "\n",
    "else:\n",
    "    pred_df_metamap = filter_pred(pred_temp_df, remove_non_asd=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter true df and add CUI\n",
    "true_df_metamap = filter_true(true_temp_df, gold_standard, gold_standard_cui_to_tui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate predictions on same entity span\n",
    "pred_df_metamap = pred_df_metamap.drop_duplicates(subset=[\"paper\", \"Start\", \"End\"])\n",
    "true_df_metamap = true_df_metamap.drop_duplicates(subset=[\"paper\", \"Start\", \"End\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamap_entities = pred_df_metamap[\"Entity\"]\n",
    "print(\"MetaMap mean entity no. of words =\", np.mean([len(str(ent).split(\" \")) for ent in metamap_entities]))\n",
    "print(\"MetaMap mean entity no. of words =\", np.std([len(str(ent).split(\" \")) for ent in metamap_entities]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df_metamap_small = true_df_metamap[[\"Entity\", \"TUI\", \"Start\", \"End\", \"paper\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_metamap_small = pred_df_metamap[[\"Entity\", \"TUI\", \"Start\", \"End\", \"paper\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results for metamap\n",
    "print(\"MetaMap results:\")\n",
    "metamap_true_pos_df = calculate_statistics(pred_df_metamap_small, true_df_metamap_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze true and false positive, and false negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_grouped, false_pos_grouped, false_neg_grouped, false_pos, false_neg = get_false_and_true_pos(metamap_true_pos_df, pred_df_metamap, true_df_metamap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER:\n",
    "    filtered = \"filtered_\"\n",
    "else:\n",
    "    filtered = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metamap_true_pos_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamap_true_pos_df.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, filtered + \"metamap_true_positive.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_grouped.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, filtered + \"metamap_false_positive.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_grouped.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, filtered + \"metamap_false_negative.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels from PLOS ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT_DIRECTORY = \"pubmed_all_fulltext_input\"\n",
    "#INPUT_DIRECTORY = \"pubmed_abstracts_24860\"\n",
    "\n",
    "INPUT_DIRECTORY = \"full_text_train_test/test\"\n",
    "#INPUT_DIRECTORY = \"abstract_train_test/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true/labelled terms/entities\n",
    "\n",
    "i = 0\n",
    "true_temp_df = pd.DataFrame()\n",
    "for idx, filename in enumerate(os.listdir(INPUT_DIRECTORY)):\n",
    "\n",
    "    if filename.endswith(\".txt\"):\n",
    "        path = os.path.join(INPUT_DIRECTORY, filename)\n",
    "        \n",
    "        if idx % 5 == 0:\n",
    "            print(idx, filename)\n",
    "        \n",
    "        # tag entities in abstract\n",
    "        with open(path, \"r\") as f:\n",
    "            data = f.read()\n",
    "\n",
    "        doc = nlp(data)\n",
    "        matches = matcher(doc)\n",
    "        spans = []\n",
    "        \n",
    "        for match_id, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            spans.append(span)\n",
    "            \n",
    "        filtered = spacy.util.filter_spans(spans)\n",
    "        \n",
    "        for span in filtered:\n",
    "            ent = span.text.lower().strip()\n",
    "            temp = pd.DataFrame({\"Entity\": span.text, \"Entity_lower\": ent, \"paper\": filename, \"Start\": span.start_char, \"End\":span.end_char, \"Sentence\":span.sent.text}, index=[i])\n",
    "            true_temp_df = true_temp_df.append(temp)\n",
    "            i = i + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true_temp_df.to_csv(\"full_text_labels_df.csv\")\n",
    "#true_temp_df.to_csv(\"abstract_labels_df.csv\")\n",
    "\n",
    "true_temp_df.to_csv(\"full_text_labels_test_df_spacy.csv\")\n",
    "#true_temp_df.to_csv(\"abstract_labels_test_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here to load true labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTRACT = False\n",
    "\n",
    "if ABSTRACT:\n",
    "    LABELLED_DIR = os.path.join(\"abstract_train_test\", \"labelled_text\", \"test\")\n",
    "else:\n",
    "    LABELLED_DIR = os.path.join(\"full_text_train_test\", \"labelled_text\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true/labelled terms/entities\n",
    "\n",
    "i = 0\n",
    "true_temp_df = pd.DataFrame()\n",
    "for idx, filename in enumerate(os.listdir(LABELLED_DIR)):\n",
    "\n",
    "    if filename.endswith(\".txt\"):\n",
    "        path = os.path.join(LABELLED_DIR, filename)\n",
    "        \n",
    "        temp = pd.read_csv(path, sep=\"\\t\")\n",
    "        temp = temp[temp[\"Semantic\"]==\"AutismTerm\"]\n",
    "        temp = temp.drop([\"CUI\", \"Assertion\"], axis=1)\n",
    "        temp[\"Entity_lower\"] = temp[\"Entity\"].str.lower()\n",
    "        temp[\"paper\"] = filename\n",
    "        \n",
    "        true_temp_df = true_temp_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ABSTRACT:\n",
    "    true_temp_df.to_csv(\"abstract_labels_clamp_test_df.csv\")\n",
    "else:\n",
    "    true_temp_df.to_csv(\"full_text_labels_clamp_test_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here to load true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels from CLAMP dictionary match\n",
    "\n",
    "#true_temp_df = pd.read_csv(\"full_text_labels_clamp_test_df.csv\", index_col=0)\n",
    "\n",
    "#true_temp_df = pd.read_csv(\"abstract_labels_clamp_test_df.csv\", index_col=0)\n",
    "\n",
    "# spacy labels\n",
    "#true_temp_df = pd.read_csv(\"full_text_labels_test_df_spacy.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTRACT = False\n",
    "\n",
    "if ABSTRACT:\n",
    "    true_temp_df = pd.read_csv(\"abstract_labels_df.csv\", index_col=0)\n",
    "else:\n",
    "    true_temp_df = pd.read_csv(\"full_text_labels_df.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities labelled = 48366\n"
     ]
    }
   ],
   "source": [
    "true_df_full_text = filter_true(true_temp_df, gold_standard, gold_standard_cui_to_tui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_df_full_text = true_df_full_text.drop_duplicates([\"paper\", \"Start\", \"End\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48309"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_df_full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ABSTRACT:\n",
    "    grouped_true_full_text = get_results_grouped(true_df_full_text, write_to_file=False, filename=\"abstract-labels.txt\")\n",
    "    grouped_true_full_text.to_csv(\"abstract-labels.csv\")\n",
    "else:\n",
    "    grouped_true_full_text = get_results_grouped(true_df_full_text, write_to_file=False, filename=\"full-text-labels.txt\")\n",
    "    grouped_true_full_text.to_csv(\"full-text-labels.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct true entities detected (case-sensitive): 162\n",
      "Distinct true entities detected (case-insensitive): 98\n"
     ]
    }
   ],
   "source": [
    "print(\"Distinct true entities detected (case-sensitive):\", len(set(true_df_full_text[\"Entity\"])))\n",
    "print(\"Distinct true entities detected (case-insensitive):\", len(set(true_df_full_text[\"Entity_lower\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df_full_text[true_df_full_text[\"Entity_lower\"] == \"head shaking\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df_full_text = true_df_full_text.drop_duplicates([\"paper\", \"Start\", \"End\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_true_full_text.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = {}\n",
    "for i, row in grouped_true_full_text.iterrows():\n",
    "    ent = row[\"Entity\"]\n",
    "    freq = row[\"count\"]\n",
    "    freq_dict[ent] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "wc = WordCloud(background_color=\"white\", max_words=1000, width=4000, height=2000, min_font_size=10, relative_scaling=0.2, max_font_size=500)\n",
    "# generate word cloud\n",
    "wc.generate_from_frequencies(freq_dict)\n",
    "\n",
    "# show\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "FIGURES_DIR = 'figures'\n",
    "if ABSTRACT:\n",
    "    fig_title = \"word_cloud_abstract\"\n",
    "else:\n",
    "    fig_title = \"word_cloud_full_text\"\n",
    "plt.savefig(FIGURES_DIR + \"/\" + fig_title + \".png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLAMP get predictions from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAMP_DIRECTORY = \"clamp\"\n",
    "\n",
    "ABSTRACT = False\n",
    "\n",
    "if ABSTRACT:\n",
    "    CLAMP_OUTPUT_DIRECTORY = os.path.join(CLAMP_DIRECTORY, \"clamp_crf_output_abstract\") \n",
    "    CLAMP_OUTPUT_DIRECTORY_UPDATED = os.path.join(CLAMP_DIRECTORY, \"clamp_crf_output_abstract_updated\") \n",
    "    CLAMP_RESULTS_DIRECTORY = os.path.join(CLAMP_DIRECTORY, \"clamp_crf_results_abstract\") \n",
    "    INPUT_DIRECTORY = os.path.join(\"abstract_train_test\", \"test\")\n",
    "else:\n",
    "    CLAMP_OUTPUT_DIRECTORY = os.path.join(CLAMP_DIRECTORY, \"clamp_crf_output_full_text\") \n",
    "    CLAMP_OUTPUT_DIRECTORY_UPDATED = os.path.join(CLAMP_DIRECTORY, \"clamp_crf_output_full_text_updated\") \n",
    "    CLAMP_RESULTS_DIRECTORY = os.path.join(CLAMP_DIRECTORY, \"clamp_crf_results_full_text\") \n",
    "    INPUT_DIRECTORY = os.path.join(\"full_text_train_test\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity(full_text, row):\n",
    "    ent = full_text[row['Start']:row['End']]\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence(sentences, row):\n",
    "    ent_start = row['Start']\n",
    "    for s in doc.sents:\n",
    "        if s.start_char <= ent_start and ent_start < s.end_char:\n",
    "            return s.text\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only relevant predictions\n",
    "def update_clamp_output(file, file_out):\n",
    "    for line in file:\n",
    "        splits = line.split(\"\\t\")\n",
    "        if splits[0].strip() == \"NamedEntity\" and splits[3].strip() == \"semantic=AutismTerm\":\n",
    "            splits_updated = []\n",
    "            for s in splits:\n",
    "                # keep only part after \"=\"\n",
    "                if \"=\" in s:\n",
    "                    splits_updated.append(s.split(\"=\")[-1].strip())\n",
    "                else:\n",
    "                    splits_updated.append(s)\n",
    "            file_out.write(\"\\t\".join(splits_updated) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(CLAMP_OUTPUT_DIRECTORY):\n",
    "    \n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        file_in_path = os.path.join(CLAMP_OUTPUT_DIRECTORY, filename)\n",
    "        file_out_path = os.path.join(CLAMP_OUTPUT_DIRECTORY_UPDATED, filename)\n",
    "        \n",
    "        with open(file_in_path) as file_in:\n",
    "            with open(file_out_path, \"w\") as file_out:\n",
    "                update_clamp_output(file_in, file_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "pred_df_original = pd.DataFrame()\n",
    "for filename in os.listdir(CLAMP_OUTPUT_DIRECTORY_UPDATED):\n",
    "    \n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(idx, filename)\n",
    "            \n",
    "        # ignore empty files\n",
    "        if is_file_empty(CLAMP_OUTPUT_DIRECTORY_UPDATED, filename):\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(INPUT_DIRECTORY, filename)) as f:\n",
    "            full_text = f.read()\n",
    "            doc = nlp(full_text)\n",
    "        \n",
    "        temp = pd.read_csv(os.path.join(CLAMP_OUTPUT_DIRECTORY_UPDATED, filename), sep=\"\\t\", quoting=3, header=None, error_bad_lines=False)\n",
    "        temp.columns = [\"type\", \"Start\", \"End\", \"semantic\", \"assertion\", \"CUI\", \"sentProb\", \"conceptProb\", \"Entity\"]\n",
    "        temp[\"paper\"] = filename\n",
    "        temp[\"Entity_matched\"] = list(temp.apply(lambda row: extract_entity(full_text, row), axis=1))\n",
    "        temp[\"Sentence_pred\"] = temp.apply(lambda row: extract_sentence(doc.sents, row), axis=1)\n",
    "        \n",
    "        pred_df_original = pred_df_original.append(temp)\n",
    "        \n",
    "        idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_temp = pred_df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_temp.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, \"clamp_preds.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLAMP get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clamp input and output\n",
    "CLAMP_DIRECTORY = \"clamp\"\n",
    "\n",
    "INPUT_DIRECTORY_FULL_TEXT = \"pubmed_all_fulltext_input\"\n",
    "INPUT_DIRECTORY_ABSTRACT = \"pubmed_abstracts_24860\"\n",
    "\n",
    "CLAMP_OUTPUT_DIRECTORY_FULL_TEXT = os.path.join(CLAMP_DIRECTORY, \"clamp_output_full_text\") \n",
    "CLAMP_OUTPUT_DIRECTORY_ABSTRACT = os.path.join(CLAMP_DIRECTORY, \"clamp_output_abstract\") \n",
    "\n",
    "CLAMP_RESULTS_DIRECTORY_FULL_TEXT = os.path.join(CLAMP_DIRECTORY, \"clamp_results_full_text\")\n",
    "CLAMP_RESULTS_DIRECTORY_ABSTRACT = os.path.join(CLAMP_DIRECTORY, \"clamp_results_abstract\")\n",
    "\n",
    "# choose whether to use full-text or abstract - MODIFY HERE\n",
    "ABSTRACT = True\n",
    "\n",
    "if ABSTRACT:\n",
    "    CLAMP_OUTPUT_DIRECTORY = CLAMP_OUTPUT_DIRECTORY_ABSTRACT\n",
    "    CLAMP_RESULTS_DIRECTORY = CLAMP_RESULTS_DIRECTORY_ABSTRACT \n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_ABSTRACT\n",
    "else:\n",
    "    CLAMP_OUTPUT_DIRECTORY = CLAMP_OUTPUT_DIRECTORY_FULL_TEXT\n",
    "    CLAMP_RESULTS_DIRECTORY = CLAMP_RESULTS_DIRECTORY_FULL_TEXT\n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_FULL_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity(full_text, row):\n",
    "    ent = full_text[row['Start']:row['End']]\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence(sentences, row):\n",
    "    ent_start = row['Start']\n",
    "    for s in doc.sents:\n",
    "        if s.start_char <= ent_start and ent_start < s.end_char:\n",
    "            return s.text\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "pred_df_original = pd.DataFrame()\n",
    "for filename in os.listdir(CLAMP_OUTPUT_DIRECTORY):\n",
    "    \n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        if idx % 500 == 0:\n",
    "            print(idx, filename)\n",
    "        \n",
    "        with open(os.path.join(INPUT_DIRECTORY, filename)) as f:\n",
    "            full_text = f.read()\n",
    "            doc = nlp(full_text)\n",
    "        \n",
    "        temp = pd.read_csv(os.path.join(CLAMP_OUTPUT_DIRECTORY, filename), sep=\"\\t\", quoting=3)\n",
    "        temp[\"paper\"] = filename\n",
    "        temp[\"Entity_matched\"] = list(temp.apply(lambda row: extract_entity(full_text, row), axis=1))\n",
    "        temp[\"Sentence_pred\"] = temp.apply(lambda row: extract_sentence(doc.sents, row), axis=1)\n",
    "        \n",
    "        pred_df_original = pred_df_original.append(temp)\n",
    "        \n",
    "        idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_temp = pred_df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_temp.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, \"clamp_preds.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here to load CLAMP predictions and output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clamp input and output\n",
    "CLAMP_DIRECTORY = \"clamp\"\n",
    "\n",
    "INPUT_DIRECTORY_FULL_TEXT = \"pubmed_all_fulltext_input\"\n",
    "INPUT_DIRECTORY_ABSTRACT = \"pubmed_abstracts_24860\"\n",
    "\n",
    "CLAMP_OUTPUT_DIRECTORY_FULL_TEXT = os.path.join(CLAMP_DIRECTORY, \"clamp_output_full_text\") \n",
    "CLAMP_OUTPUT_DIRECTORY_ABSTRACT = os.path.join(CLAMP_DIRECTORY, \"clamp_output_abstract\") \n",
    "\n",
    "CLAMP_RESULTS_DIRECTORY_FULL_TEXT = os.path.join(CLAMP_DIRECTORY, \"clamp_results_full_text\")\n",
    "CLAMP_RESULTS_DIRECTORY_ABSTRACT = os.path.join(CLAMP_DIRECTORY, \"clamp_results_abstract\")\n",
    "\n",
    "# choose whether to use full-text or abstract - MODIFY HERE\n",
    "ABSTRACT = True\n",
    "\n",
    "if ABSTRACT:\n",
    "    CLAMP_OUTPUT_DIRECTORY = CLAMP_OUTPUT_DIRECTORY_ABSTRACT\n",
    "    CLAMP_RESULTS_DIRECTORY = CLAMP_RESULTS_DIRECTORY_ABSTRACT \n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_ABSTRACT\n",
    "else:\n",
    "    CLAMP_OUTPUT_DIRECTORY = CLAMP_OUTPUT_DIRECTORY_FULL_TEXT\n",
    "    CLAMP_RESULTS_DIRECTORY = CLAMP_RESULTS_DIRECTORY_FULL_TEXT\n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_FULL_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER = True # filter some predicted terms to increase precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for getting results of trained model\n",
    "\n",
    "# CLAMP_DIRECTORY = \"clamp\"\n",
    "\n",
    "# ABSTRACT = False\n",
    "\n",
    "# FILTER = False\n",
    "\n",
    "# if ABSTRACT:\n",
    "#     CLAMP_OUTPUT_DIRECTORY = os.path.join(CLAMP_DIRECTORY, \"clamp_crf_output_abstract\") \n",
    "#     CLAMP_OUTPUT_DIRECTORY_UPDATED = os.path.join(CLAMP_DIRECTORY, \"clamp_crf_output_abstract_updated\") \n",
    "#     CLAMP_RESULTS_DIRECTORY = os.path.join(CLAMP_DIRECTORY, \"clamp_crf_results_abstract\") \n",
    "#     INPUT_DIRECTORY = os.path.join(\"abstract_train_test\", \"test\")\n",
    "# else:\n",
    "#     CLAMP_OUTPUT_DIRECTORY = os.path.join(CLAMP_DIRECTORY, \"clamp_crf_output_full_text\") \n",
    "#     CLAMP_OUTPUT_DIRECTORY_UPDATED = os.path.join(CLAMP_DIRECTORY, \"clamp_crf_output_full_text_updated\") \n",
    "#     CLAMP_RESULTS_DIRECTORY = os.path.join(CLAMP_DIRECTORY, \"clamp_crf_results_full_text\") \n",
    "#     INPUT_DIRECTORY = os.path.join(\"full_text_train_test\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_temp = pd.read_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, \"clamp_preds.csv\"), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER:\n",
    "    pred_df_temp = pred_df_temp[pred_df_temp[\"Semantic\"]==\"problem\"] # keep only terms where semantic is \"problem\"#\n",
    "    pred_df_temp = pred_df_temp.dropna(subset=[\"CUI\"]) # keep only terms with CUI\n",
    "\n",
    "pred_df_temp[\"CUI\"] = pred_df_temp[\"CUI\"].apply(lambda x: get_CUI(x)) # get CUI\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter predictions\n",
    "# map CUI to TUI\n",
    "cui_to_tui_map = pd.read_csv(\"clamp_cui_to_tui_map.txt\", sep=\"\\t\", header = None)\n",
    "cui_to_tui_map.columns = [\"CUI\", \"TUI\"]\n",
    "\n",
    "if FILTER:\n",
    "\n",
    "    # filter predicted terms for relevant semtypes/TUIs\n",
    "    #relevant_tuis = [\"T184\", \"T048\", \"T028\", \"T052\", \"T041\", \"T080\", \"T054\", \"T058\", \"T101\", \"T056\", \"T047\", \"T033\", \"T055\"]\n",
    " \n",
    "    relevant_tuis = [\"T033\", \"T048\"]\n",
    "    \n",
    "    pred_temp_df_filtered = pred_df_temp.merge(cui_to_tui_map, on=\"CUI\")\n",
    "    #pred_temp_df_filtered = pred_temp_df_filtered[(pred_temp_df_filtered[\"TUI\"].isin(relevant_tuis))]\n",
    "    pred_temp_df_filtered = pred_temp_df_filtered[(pred_temp_df_filtered[\"TUI\"].isin(relevant_tuis)) | (pred_temp_df_filtered[\"Entity\"].str.contains(\"ASD\"))]\n",
    "\n",
    "    # correct CUI\n",
    "    #pred_temp_df_filtered = pred_temp_df_filtered.replace({'CUI': 'C0018817'}, \"C1510586\")\n",
    "    \n",
    "    pred_df_clamp = filter_pred(pred_temp_df_filtered, remove_non_asd=True) # use clamp preds filtered by semtype\n",
    "\n",
    "else:\n",
    "    pred_df_temp = pred_df_temp.merge(cui_to_tui_map, on=\"CUI\", how=\"left\")\n",
    "    pred_df_clamp = filter_pred(pred_df_temp, remove_non_asd=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_clamp = pred_df_clamp.drop_duplicates([\"Start\", \"End\", \"paper\"])\n",
    "true_df_full_text = true_df_full_text.drop_duplicates([\"Start\", \"End\", \"paper\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clamp_entities = pred_df_clamp[\"Entity\"]\n",
    "print(\"CLAMP mean entity no. of words =\", np.mean([len(str(ent).split(\" \")) for ent in clamp_entities]))\n",
    "print(\"CLAMP mean entity no. of words =\", np.std([len(str(ent).split(\" \")) for ent in clamp_entities]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CLAMP results:\")\n",
    "clamp_true_pos_df = calculate_statistics(pred_df_clamp, true_df_full_text, match_cui=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze true and false positive, and false negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_grouped, false_pos_grouped, false_neg_grouped, false_pos, false_neg = get_false_and_true_pos(clamp_true_pos_df, pred_df_clamp, true_df_full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER:\n",
    "    filtered = \"filtered_\"\n",
    "else:\n",
    "    filtered = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_grouped.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, filtered + \"clamp_true_positive.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_grouped.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, filtered + \"clamp_false_positive.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_grouped.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, filtered + \"clamp_false_negative.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cTakes Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctakes input and output\n",
    "CTAKES_DIRECTORY = \"ctakes\"\n",
    "\n",
    "INPUT_DIRECTORY_FULL_TEXT = \"pubmed_all_fulltext_input\"\n",
    "INPUT_DIRECTORY_ABSTRACT = \"pubmed_abstracts_24860\"\n",
    "\n",
    "CTAKES_OUTPUT_DIRECTORY_FULL_TEXT = os.path.join(CTAKES_DIRECTORY, \"ctakes_output_full_text\") \n",
    "CTAKES_OUTPUT_DIRECTORY_ABSTRACT = os.path.join(CTAKES_DIRECTORY, \"ctakes_output_abstract\") \n",
    "\n",
    "CTAKES_RESULTS_DIRECTORY_FULL_TEXT = os.path.join(CTAKES_DIRECTORY, \"ctakes_results_full_text\")\n",
    "CTAKES_RESULTS_DIRECTORY_ABSTRACT = os.path.join(CTAKES_DIRECTORY, \"ctakes_results_abstract\")\n",
    "\n",
    "# choose whether to use full-text or abstract - MODIFY HERE\n",
    "ABSTRACT = True\n",
    "\n",
    "if ABSTRACT:\n",
    "    CTAKES_OUTPUT_DIRECTORY = CTAKES_OUTPUT_DIRECTORY_ABSTRACT\n",
    "    CTAKES_RESULTS_DIRECTORY = CTAKES_RESULTS_DIRECTORY_ABSTRACT \n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_ABSTRACT\n",
    "else:\n",
    "    CTAKES_OUTPUT_DIRECTORY = CTAKES_OUTPUT_DIRECTORY_FULL_TEXT\n",
    "    CTAKE_RESULTS_DIRECTORY = CTAKES_RESULTS_DIRECTORY_FULL_TEXT\n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_FULL_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile predicted entities for ctakes\n",
    "\n",
    "pred_df_temp = pd.DataFrame()\n",
    "i = 0\n",
    "for filename in os.listdir(CTAKES_OUTPUT_DIRECTORY):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        \n",
    "        # ignore empty files\n",
    "        if is_file_empty(CTAKES_OUTPUT_DIRECTORY, filename):\n",
    "            continue\n",
    "        \n",
    "        if ABSTRACT:\n",
    "            filename_updated = filename.replace(\".txt\", \"\")\n",
    "        else:\n",
    "            filename_updated = filename\n",
    "            \n",
    "        if i % 5 == 0:\n",
    "            print(i, filename_updated)\n",
    "        \n",
    "        input_filename = filename_updated.replace(\".csv\", \".txt\")\n",
    "        \n",
    "        if input_filename == \"PMC6061181.txt\": # skip this paper\n",
    "            continue\n",
    "             \n",
    "        with open(os.path.join(INPUT_DIRECTORY, input_filename)) as f:\n",
    "            full_text = f.read()       \n",
    "          \n",
    "        temp = pd.read_csv(os.path.join(CTAKES_OUTPUT_DIRECTORY, filename))\n",
    "        temp[\"paper\"] = filename.replace(\".csv\", \".txt\")\n",
    "        temp = temp.rename(columns={\"cui\":\"CUI\", \"tui\":\"TUI\", \"pos_start\":\"Start\", \"pos_end\":\"End\"})\n",
    "        temp[\"Entity\"] = temp.apply(lambda row: full_text[row['Start']:row['End']], axis=1)\n",
    "        pred_df_temp = pred_df_temp.append(temp)\n",
    "        \n",
    "\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_temp.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, \"ctakes_preds.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start here to load cTAKES results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctakes input and output\n",
    "CTAKES_DIRECTORY = \"ctakes\"\n",
    "\n",
    "INPUT_DIRECTORY_FULL_TEXT = \"pubmed_all_fulltext_input\"\n",
    "INPUT_DIRECTORY_ABSTRACT = \"pubmed_abstracts_24860\"\n",
    "\n",
    "CTAKES_OUTPUT_DIRECTORY_FULL_TEXT = os.path.join(CTAKES_DIRECTORY, \"ctakes_output_full_text\") \n",
    "CTAKES_OUTPUT_DIRECTORY_ABSTRACT = os.path.join(CTAKES_DIRECTORY, \"ctakes_output_abstract\") \n",
    "\n",
    "CTAKES_RESULTS_DIRECTORY_FULL_TEXT = os.path.join(CTAKES_DIRECTORY, \"ctakes_results_full_text\")\n",
    "CTAKES_RESULTS_DIRECTORY_ABSTRACT = os.path.join(CTAKES_DIRECTORY, \"ctakes_results_abstract\")\n",
    "\n",
    "# choose whether to use full-text or abstract - MODIFY HERE\n",
    "ABSTRACT = True\n",
    "\n",
    "if ABSTRACT:\n",
    "    CTAKES_OUTPUT_DIRECTORY = CTAKES_OUTPUT_DIRECTORY_ABSTRACT\n",
    "    CTAKES_RESULTS_DIRECTORY = CTAKES_RESULTS_DIRECTORY_ABSTRACT \n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_ABSTRACT\n",
    "else:\n",
    "    CTAKES_OUTPUT_DIRECTORY = CTAKES_OUTPUT_DIRECTORY_FULL_TEXT\n",
    "    CTAKES_RESULTS_DIRECTORY = CTAKES_RESULTS_DIRECTORY_FULL_TEXT\n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_FULL_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_temp = pd.read_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, \"ctakes_preds.csv\"), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER:\n",
    "    # filter predicted terms for relevant semtypes/TUIs\n",
    "    relevant_tuis = [\"T048\", \"T033\"]\n",
    "    \n",
    "    pred_temp_df_filtered = pred_df_temp\n",
    "    pred_temp_df_filtered = pred_temp_df_filtered[(pred_temp_df_filtered[\"TUI\"].isin(relevant_tuis)) | (pred_temp_df_filtered[\"Entity\"].str.contains(\"ASD\"))]\n",
    "    \n",
    "    relevant_textsem = [\"DiseaseDisorderMention\", \"SignSymptomMention\"]\n",
    "    pred_temp_df_filtered = pred_temp_df_filtered[pred_temp_df_filtered[\"textsem\"].isin(relevant_textsem)]\n",
    "    \n",
    "    pred_df_ctakes = filter_pred(pred_temp_df_filtered, remove_non_asd=True) # use ctakes preds filtered by semtype\n",
    "\n",
    "else:\n",
    "    pred_df_ctakes = filter_pred(pred_df_temp, remove_non_asd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_ctakes = pred_df_ctakes.drop_duplicates([\"Start\", \"End\", \"paper\"])\n",
    "true_df_full_text = true_df_full_text.drop_duplicates([\"Start\", \"End\", \"paper\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctakes_entities = pred_df_ctakes[\"Entity\"]\n",
    "print(\"cTAKES mean entity no. of words =\", np.mean([len(str(ent).split(\" \")) for ent in ctakes_entities]))\n",
    "print(\"cTAKES mean entity no. of words =\", np.std([len(str(ent).split(\" \")) for ent in ctakes_entities]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ABSTRACT:\n",
    "    pred_df_ctakes[\"paper\"] = pred_df_ctakes[\"paper\"].str.replace(\".txt\", \"\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_ctakes[\"Entity\"] = pred_df_ctakes[\"true_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cTAKES results:\")\n",
    "ctakes_true_pos_df = calculate_statistics(pred_df_ctakes, true_df_full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze true and false positive, and false negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_grouped, false_pos_grouped, false_neg_grouped, false_pos, false_neg = get_false_and_true_pos(ctakes_true_pos_df, pred_df_ctakes, true_df_full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER:\n",
    "    filtered = \"filtered_\"\n",
    "else:\n",
    "    filtered = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_grouped.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, filtered + \"ctakes_true_positive.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_grouped.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, filtered + \"ctakes_false_positive.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_grouped.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, filtered + \"ctakes_false_negative.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioBERT results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biobert input and output\n",
    "BIOBERT_DIRECTORY = \"biobert\"\n",
    "\n",
    "INPUT_DIRECTORY_FULL_TEXT = \"pubmed_all_fulltext_input\"\n",
    "INPUT_DIRECTORY_ABSTRACT = \"pubmed_abstracts_24860\"\n",
    "\n",
    "BIOBERT_OUTPUT_DIRECTORY_FULL_TEXT = os.path.join(BIOBERT_DIRECTORY, \"biobert_output_full_text\") \n",
    "BIOBERT_OUTPUT_DIRECTORY_ABSTRACT = os.path.join(BIOBERT_DIRECTORY, \"biobert_output_abstract\") \n",
    "\n",
    "BIOBERT_RESULTS_DIRECTORY_FULL_TEXT = os.path.join(BIOBERT_DIRECTORY, \"biobert_results_full_text\")\n",
    "BIOBERT_RESULTS_DIRECTORY_ABSTRACT = os.path.join(BIOBERT_DIRECTORY, \"biobert_results_abstract\")\n",
    "\n",
    "# choose whether to use full-text or abstract - MODIFY HERE\n",
    "ABSTRACT = True\n",
    "\n",
    "if ABSTRACT:\n",
    "    BIOBERT_OUTPUT_DIRECTORY = BIOBERT_OUTPUT_DIRECTORY_ABSTRACT\n",
    "    BIOBERT_RESULTS_DIRECTORY = BIOBERT_RESULTS_DIRECTORY_ABSTRACT \n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_ABSTRACT\n",
    "else:\n",
    "    BIOBERT_OUTPUT_DIRECTORY = BIOBERT_OUTPUT_DIRECTORY_FULL_TEXT\n",
    "    BIOBERT_RESULTS_DIRECTORY = BIOBERT_RESULTS_DIRECTORY_FULL_TEXT\n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_FULL_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_temp = pd.read_csv(os.path.join(BIOBERT_RESULTS_DIRECTORY, \"biobert_preds.csv\"), index_col=0)\n",
    "true_df_full_text = pd.read_csv(os.path.join(BIOBERT_RESULTS_DIRECTORY, \"biobert_labels.csv\"), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_temp[pred_df_temp[\"paper\"].str.contains(\"PMC6061181\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_biobert = filter_pred(pred_df_temp, remove_non_asd=False, cui=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_biobert = pred_df_biobert.drop_duplicates([\"Start\", \"End\", \"paper\"])\n",
    "true_df_full_text = true_df_full_text.drop_duplicates([\"Start\", \"End\", \"paper\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biobert_entities = pred_df_biobert[\"Entity\"]\n",
    "print(\"BIOBERT mean entity no. of words =\", np.mean([len(str(ent).split(\" \")) for ent in biobert_entities]))\n",
    "print(\"BIOBERT mean entity no. of words =\", np.std([len(str(ent).split(\" \")) for ent in biobert_entities]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BIOBERT results:\")\n",
    "biobert_true_pos_df = calculate_statistics(pred_df_biobert, true_df_full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze true and false positive, and false negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_grouped, false_pos_grouped, false_neg_grouped, false_pos, false_neg = get_false_and_true_pos(biobert_true_pos_df, pred_df_biobert, true_df_full_text, cui=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_grouped.to_csv(os.path.join(BIOBERT_RESULTS_DIRECTORY, \"biobert_true_positive.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_grouped.to_csv(os.path.join(BIOBERT_RESULTS_DIRECTORY, \"biobert_false_positive.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_grouped.to_csv(os.path.join(BIOBERT_RESULTS_DIRECTORY, \"biobert_false_negative.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
